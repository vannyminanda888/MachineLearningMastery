# ğŸš€Customer Satisfaction Prediction

## ğŸ« Goal : Will This Passenger Be Happy? Let's Find Out!

Ever wondered what makes someone love or hate their flight? In this project, I take on the role of a data detective ğŸ•µï¸â€â™€ï¸, digging into real airline survey data to predict whether a future passenger will leave the plane smiling â€” or swearing never to fly that airline again.

The dataset includes feedback from 129,880 travelers, covering everything from legroom complaints to in-flight entertainment scores ğŸ¿âœˆï¸. My mission? Build a model that not only predicts satisfaction but also reveals which flight features matter most to passengers.

Think of it as turning air turbulence into clean, predictive insights.

## ğŸ“¦ Dataset Overview
The dataset used is Invistico_Airline.csv, containing features such as:
* Travel type & class
* Flight distance
* In-flight services (entertainment, wifi, food, etc.)
* Customer type (loyal vs. disloyal)
* Delay times and more!
### Target variable: Satisfaction (satisfied or neutral/dissatisfied)

## ğŸ§¹ Data Preprocessing & Cleaning
#### Before modeling, I took the time to clean things up:
* Removed rows with missing or null values.
* Encoded categorical features using LabelEncoder and OneHotEncoder where appropriate.
* Scaled numerical features like flight distance and delay times for better model performance.
* Checked for class imbalance and ensured fair model evaluation.

## ğŸ“Š Exploratory Data Analysis (EDA)
* Explored distributions of flight class, travel type, and satisfaction.
* Found that business class passengers and loyal customers tend to report higher satisfaction.
* Delays and poor in-flight services strongly correlate with dissatisfaction.

## ğŸ¤– Modeling & Results
#### ğŸ” Models Built:
* Random Forest without CV
* Random Forest with GridSearchCV for Hyperparameter Tuning

## ğŸ Results:
After training and evaluating multiple models, here's how they stacked up:

### ğŸŒ³ Random Forest without Hyperparameter Tuning:
Accuracy: 95.7% â€” This looks impressive at first glance. However, it's important to note that this result might be overly optimistic, possibly due to the test set being particularly well-suited for this model by chance. Without cross-validation, the performance may not be as reliable across different data splits.

### ğŸŒ²ğŸŒ² Random Forest with GridSearchCV:
Accuracy: 94.2% â€” Although slightly lower in raw accuracy, this model is more stable and representative of true performance. By incorporating cross-validation and hyperparameter tuning, this model is better equipped to generalize to unseen data, making it the more trustworthy of the two.

#### ğŸ” Key takeaway: Sometimes, higher accuracy doesn't mean better performance. A well-tuned model with cross-validation provides a more robust and dependable benchmark, even if the raw score appears slightly lower.


